import torch
import pandas as pd
import numpy as np
import ast

# Import Hugging Face libraries for tokenization and model
from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments

# Load the dataset
df = pd.read_csv('annotated_sentences.csv')

# Initialize the tokenizer and model from the pre-trained 'bert-base-uncased' model
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Mapping of entity tags to label IDs 
tag2id = {'O': 0, 'B-MOUNTAIN': 1, 'I-MOUNTAIN': 2}
id2tag = {v: k for k, v in tag2id.items()}  # Reverse mapping from ID to tag

# Convert token and tag columns to Python objects (lists) from strings in the CSV
df['tokens'] = df['tokens'].apply(ast.literal_eval)
df['tags'] = df['tags'].apply(ast.literal_eval)

# Function to align the labels with the sub-tokens created by the tokenizer
def align_labels_with_tokens(labels, word_ids):
    """
    Align the word labels with the sub-tokens generated by the tokenizer.
    Special tokens or sub-tokens are given a label of -100 to exclude them from loss calculation.
    """
    new_labels = [-100] * len(word_ids)  # Initialize all labels to -100 (special tokens or sub-tokens)
    label_index = 0  
    for i, word_id in enumerate(word_ids):
        if word_id is not None:  # Skip special tokens (e.g., [CLS], [SEP])
            if label_index < len(labels):
                new_labels[i] = labels[label_index]  
            if i == 0 or word_id != word_ids[i - 1]:
                label_index += 1
    return new_labels

# Tokenize the sentences and align them with the labels
tokenized_inputs = tokenizer(
    df['tokens'].tolist(),  
    is_split_into_words=True,  
    padding=True, 
    truncation=True,  
    return_offsets_mapping=True,  
    max_length=512 
)

# Align the labels with the tokenized inputs
labels_aligned = []
for i in range(len(df)):
    word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get word IDs for sub-tokens
    labels = [tag2id[tag] for tag in df['tags'][i]]  # Convert text to numeric labels
    labels_aligned.append(align_labels_with_tokens(labels, word_ids))  # Align the labels with the tokens

# Add the aligned labels to the tokenized inputs
tokenized_inputs['labels'] = labels_aligned

# Define a custom dataset class for use with the Hugging Face Trainer API
class NERDataset(torch.utils.data.Dataset):
    """
    Dataset class compatible with the Hugging Face Trainer API.
    """
    def __init__(self, encodings):
        self.encodings = encodings  

    def __getitem__(self, idx):
        # Convert all keys in the encoding to PyTorch tensors for model input
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

# Create a dataset object using the tokenized inputs
dataset = NERDataset(tokenized_inputs)

# Set up training arguments for the Trainer API
training_args = TrainingArguments(
    output_dir='./results',          
    num_train_epochs=7,             
    per_device_train_batch_size=16,  
    warmup_steps=500,               
    weight_decay=0.01,              
    logging_dir='./logs',          
    learning_rate=2e-5,             
    evaluation_strategy='no',       
    save_strategy='no',             
    logging_steps=10,               
)

# Create a Trainer instance for managing the training process
trainer = Trainer(
    model=model,               
    args=training_args,        
    train_dataset=dataset,      
)

# Start training the model
trainer.train()

# Save the trained model and tokenizer to disk
output_dir = "./model_save"
model.save_pretrained(output_dir)  # Save the model weights and configuration
tokenizer.save_pretrained(output_dir)  # Save the tokenizer configuration and vocabulary